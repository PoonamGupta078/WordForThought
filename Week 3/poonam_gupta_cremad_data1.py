# -*- coding: utf-8 -*-
"""Poonam_Gupta_CremaD_data1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Uslfahxs5sQXgbpcBBvYDm8bTI79rnbx

Importing Important Libraries
"""

import kagglehub
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
import warnings
warnings.filterwarnings('ignore')
import librosa
import librosa.display
import IPython.display as ipd
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from keras.utils import to_categorical
import keras
from keras.callbacks import ReduceLROnPlateau
from keras.models import Sequential
from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization
import tensorflow as tf
from IPython.display import Audio
import random
from tensorflow.keras import layers, models
from sklearn.metrics import accuracy_score, classification_report
!pip install --upgrade librosa
print(librosa.__version__)
import soundfile as sf
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
import cv2

"""Load The dataset"""

path = kagglehub.dataset_download("ejlok1/cremad")
print("Files in the downloaded dataset directory:")
print(os.listdir(path))
for root, dirs, files in os.walk(path):
    print(f"Root: {root}")
    print("Files:", files)
    print("Directories:", dirs)
    print("-" * 40)
#Define the path to the AudioWAV subdirectory
audio_folder = Path(path) / "AudioWAV"

# List all .wav files in the AudioWAV folder
audio_files = list(audio_folder.glob("*.wav"))
print(f"Number of audio files found: {len(audio_files)}")
print("Some example files:", audio_files[0])

"""Check the audio file"""

audio_data, sr = librosa.load(str(audio_files[0]), sr=14300)
duration=librosa.get_duration(y=audio_data, sr=sr)
print(f"Duration of the audio file: {duration} seconds")
Audio(audio_data, rate=sr)

"""Now read the metadata file"""

data = []
for file in audio_files:
    file_name = file.name
    try:
        # Split the filename to get metadata
        actor_id, emotion, intensity = file_name.split("_")[:3]
        data.append({
            "file_path": str(file),
            "actor_id": actor_id,
            "intensity": emotion,
            "emotion": intensity.split(".")[0] # i think this data has some error because in emotion folder there is intensity so i rechange it
        })
    except ValueError:
        print(f"Error parsing filename: {file_name}")
df = pd.DataFrame(data)

# Check the first five rows of the DataFrame
print(df.head())
print(df.columns)

print(df['emotion'].unique())

emotion_mapping = {
    "ANG": 0,  # Anger
    "DIS": 1,  # Disgust
    "FEA": 2,  # Fear
    "HAP": 3,  # Happiness
    "NEU": 4,  # Neutral
    "SAD": 5   # Sadness
}
df['emotion_label'] = df['emotion'].map(emotion_mapping)
print(df.columns)
print(len(df['emotion_label']))
print(len(audio_files))

"""Now listen some interesting and fascinating emotions"""

class AudioUtil():
  @staticmethod
  def open(audio_files):
    sig, sr = librosa.load(audio_files, sr=None)  # sr=None to retain original sample rate
    return (sig, sr)
audio_files=audio_files
for audio_file in audio_files[0:5]:
        signal, sample_rate = AudioUtil.open(audio_file)
        print(f"Loaded with sample rate: {sample_rate}")
        display(Audio(signal, rate=sample_rate))

"""Convert to stereo channels so our model have the same dimensions"""

class AudioUtil:
    @staticmethod
    def rechannel(audio_file, new_channel):
        # Ensure audio_file is a string
        audio_file = str(audio_file)
        print(audio_file[0])
        sig, sr = librosa.load(audio_file, sr=None, mono=False)  # mono=False to keep channels

        if sig.ndim == 1:  # Mono (1D array)
            sig = np.expand_dims(sig, axis=0)  # Make it a 2D array (1 channel)
            sig = np.tile(sig, (2, 1))  # Convert mono to stereo
            print(f"Converted mono to stereo. New shape: {sig.shape}")

        elif sig.shape[0] == 2:  # Stereo (already 2 channels)
            print("Audio is already stereo.")
            pass  # Already stereo, no changes needed

        return sig, sr
    @staticmethod
    def process_multiple_files(audio_files, new_channel):
        processed_audio = []
        for audio_file in audio_files:
            sig, sr = AudioUtil.rechannel(audio_file, new_channel)
            if sig is not None:
                processed_audio.append((sig, sr))
        return processed_audio

"""Now check it"""

processed_audio = AudioUtil.process_multiple_files(audio_files, new_channel=2)
print(f"Number of processed files: {len(processed_audio)}")
if len(processed_audio) > 0:
    print(f"Shape of the first processed signal: {processed_audio[0][0].shape}")

"""Visualisation through waveform"""

plt.figure(figsize=(15,10))
for i in range(5):
    signal, sr = processed_audio[i]
    plt.subplot(5, 1, i + 1)
    librosa.display.waveshow(signal[0], sr=sr)
    plt.title(f"Waveform of Audio File {i + 1}")
    plt.xlabel("Time (s)")
    plt.ylabel("Amplitude")
    plt.grid(False)
    plt.show()
    print("\n")

"""Mel spectrogram


"""

def extract_mel_spectrogram(audio_file, sr=None, n_mels=128, fmax=8000):

    # Load audio file
    y, sr = librosa.load(audio_file, sr=sr)

    # Generate the Mel spectrogram
    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, fmax=fmax)

    # Convert the Mel spectrogram to log scale (log-mel)
    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram)

    return log_mel_spectrogram, sr

mel_spectrograms = []

for audio_file in audio_files:
    mel_spectrogram, sr = extract_mel_spectrogram(audio_file)
    mel_spectrograms.append(mel_spectrogram)
    print(f"Extracted Mel Spectrogram for {audio_file} with shape: {mel_spectrogram.shape}")

# Plot Mel spectrograms for the first audio file
plt.figure(figsize=(10, 4))
librosa.display.specshow(mel_spectrograms[0], x_axis='time', y_axis='mel', sr=sr, fmax=8000)
plt.title('Mel Spectrogram')
plt.tight_layout()
plt.show()

"""Custom data loader


"""

from tensorflow.keras.utils import Sequence
import numpy as np

# Custom data generator to handle variable-length spectrograms
class AudioDataGenerator(Sequence):
    def __init__(self, audio_files, labels, batch_size=32):
        self.audio_files = audio_files
        self.labels = labels
        self.batch_size = batch_size
        self.indexes = np.arange(len(self.audio_files))

    def __len__(self):
        # Number of batches per epoch
        return int(np.floor(len(self.audio_files) / self.batch_size))

    def __getitem__(self, index):
        # Generate batch of data
        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]
        batch_audio_files = [self.audio_files[k] for k in batch_indexes]
        batch_labels = [self.labels[k] for k in batch_indexes]

        # Process audio files to Mel spectrograms
        batch_spectrograms = [extract_mel_spectrogram(f)[0] for f in batch_audio_files]

        # Pad or handle variable length sequences in the generator
        max_time_steps = max([spec.shape[1] for spec in batch_spectrograms])  # Find the max time steps in the batch
        batch_spectrograms_padded = [np.pad(spec, ((0, 0), (0, max_time_steps - spec.shape[1])), mode='constant') for spec in batch_spectrograms]

        # Add an extra dimension for channels (since Conv2D expects 4D inputs)
        batch_spectrograms_padded = np.expand_dims(np.array(batch_spectrograms_padded), axis=-1)

        return batch_spectrograms_padded, np.array(batch_labels)
labels=df['emotion_label'].tolist()

train_audio_files, test_audio_files, train_labels, test_labels = train_test_split(audio_files, labels, test_size=0.2, random_state=42)
# Create training and testing generators
train_generator = AudioDataGenerator(train_audio_files, train_labels, batch_size=32)
test_generator = AudioDataGenerator(test_audio_files, test_labels, batch_size=32)

"""Create Model"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D,AveragePooling2D
from tensorflow.keras.callbacks import EarlyStopping
# Define the model

# Define LeNet-5 architecture
model = Sequential([
    # C1 - First Convolutional Layer
    Conv2D(6, kernel_size=(5, 5), activation='tanh', input_shape=(32, 32, 1)),  # Grayscale input
    # S2 - First Average Pooling Layer
    AveragePooling2D(pool_size=(2, 2)),

    # C3 - Second Convolutional Layer
    Conv2D(16, kernel_size=(5, 5), activation='tanh'),
    # S4 - Second Average Pooling Layer
    AveragePooling2D(pool_size=(2, 2)),

    # C5 - Third Convolutional Layer (fully connected to the previous layer)
    Conv2D(120, kernel_size=(5, 5), activation='tanh'),


    GlobalAveragePooling2D(),
    # F6 - First Fully Connected Layer
    Dense(84, activation='tanh'),

    # Output Layer
    Dense(10, activation='softmax')  # 10 classes for classification
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Display the model's architecture
model.summary()

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model using the data generator
history = model.fit(
    train_generator,  # Train on the data generator
    validation_data=test_generator,
    epochs=20,
    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]
)

"""Evaluating The Performance"""

test_loss, test_accuracy = model.evaluate(test_generator)

# Print the accuracy
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

train_loss, train_accuracy = model.evaluate(train_generator)

# Print the accuracy

print(f"Test Accuracy: {train_accuracy * 100:.2f}%")